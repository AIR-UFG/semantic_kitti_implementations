{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2050a67-522d-4702-8ccd-0485a6eaa400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import os, time\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaaf30f-656e-4da4-b73f-9e334fd61c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"my-awesome-project\",\n",
    "\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69777cf6-03b2-49e7-ac7f-90d8a2db5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 0.0001\n",
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 4\n",
    "N_CLASSES = 7\n",
    "MODEL_PATH = \"mvlidar.pth\"\n",
    "PLOT_PATH = \"plot.png\"\n",
    "TEST_PATHS = \"test_paths.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885da7aa-a90c-4e82-b779-c288e2535ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| eval: false\n",
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {DEVICE} device\")\n",
    "# determine if we will be pinning memory during data loading\n",
    "PIN_MEMORY = True if DEVICE == \"cuda\" else False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a2bc3-93d7-4605-aac6-2684446b263e",
   "metadata": {},
   "source": [
    "Pré processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce545e6c-17f8-49b1-9c76-aee0374fbec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# insert the path to your dataset\n",
    "train_path = ROOT_PATH+\"/train/\"\n",
    "test_path = ROOT_PATH+\"/test/\"\n",
    "\n",
    "train_merged_path = ROOT_PATH+\"/train-merged/\"\n",
    "test_merged_path = ROOT_PATH+\"/test-merged/\"\n",
    "\n",
    "data_paths = [(train_path, train_merged_path), (test_path, test_merged_path)]\n",
    "\n",
    "for data_path, merged_path in data_paths:\n",
    "    os.makedirs(merged_path, exist_ok=True)\n",
    "    api.merge_images(data_path, merged_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12880d10-5294-453d-9921-3681ad007a64",
   "metadata": {},
   "source": [
    "Pré processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1645c73a-b7c3-4bc6-8e56-8db647eb9475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "train_segmentation_mask = ROOT_PATH+\"/train_segmentation_mask/\"\n",
    "test_segmentation_mask = ROOT_PATH+\"/test_segmentation_mask/\"\n",
    "\n",
    "masks_paths = [(train_path, train_segmentation_mask), (test_path, test_segmentation_mask)]\n",
    "\n",
    "remapping_rules = {\n",
    "  1: 1,\n",
    "  4: 2,\n",
    "  6: 3,\n",
    "  7: 4,\n",
    "  2: 4,\n",
    "  9: 5,\n",
    "  11: 6\n",
    "}\n",
    "\n",
    "for data_path, mask_path in masks_paths:\n",
    "    os.makedirs(mask_path, exist_ok=True)\n",
    "    data.remap_segmentation_masks(data_path, mask_path, remapping_rules=remapping_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af34cfa3-f6a7-406e-8dec-cf99615db0b8",
   "metadata": {},
   "source": [
    "Lembrar de ajustar o nome das variaveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6669c-da48-47f3-a81d-fb144d1cbca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = data.SemanticDataset(image_path=train_imgs_path,\n",
    "                                     mask_path=train_anns_path,\n",
    "                                     transform=transform)\n",
    "\n",
    "test_dataset = data.SemanticDataset(image_path=test_imgs_path,\n",
    "                                    mask_path=test_anns_path,\n",
    "                                    transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e659248-0821-48af-8264-94ceb17ee689",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| eval: false\n",
    "trainLoader = DataLoader(train_dataset, \n",
    "                         shuffle=True, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         pin_memory=PIN_MEMORY, \n",
    "                         num_workers=os.cpu_count())\n",
    "\n",
    "testLoader = DataLoader(test_dataset, \n",
    "                        shuffle=False, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        pin_memory=PIN_MEMORY, \n",
    "                        num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556afeac-077f-4259-a272-7257c29e8bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Learner:\n",
    "    def __init__(self, lr, n_classes):\n",
    "        self.model = model.MVLidar(n_classes).to(DEVICE)\n",
    "        self.optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def update(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a14a22-932d-4cb9-8bfa-3631df5836b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        self.loss_fn = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def get_loss(self, y, y_hat):\n",
    "        bin_mask_train = (y !=0).int()\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        loss = loss * bin_mask_train\n",
    "        loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6c254-a913-44e6-8152-84cb889da804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, train_dataset, test_dataset, learner: Learner, evaluator: Evaluator, batch_size):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.learner = learner\n",
    "        self.batch_size = batch_size\n",
    "        self.evaluator = evaluator\n",
    "        self.log = defaultdict(list)\n",
    "        self.log['correct'].append(0)\n",
    "        self.log['total'].append(0)\n",
    "\n",
    "    def train(self):\n",
    "        self.learner.model.train()\n",
    "        dataloader = DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "        epoch_loss = 0\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        for batch_idx, (X,y) in enumerate(dataloader):\n",
    "            y_hat = self.learner.predict(X)\n",
    "            loss = self.evaluator.get_loss(y,y_hat)\n",
    "            epoch_loss += loss.item()\n",
    "            self.learner.update(loss)\n",
    "\n",
    "        #print(f\"avarage train loss: {epoch_loss/num_batches}\")\n",
    "        self.log['train_loss'] += [epoch_loss / num_batches]\n",
    "\n",
    "    def test(self):\n",
    "        self.learner.model.eval()\n",
    "        dataloader = DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "        epoch_loss, correct = 0 , 0\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X,y in dataloader:\n",
    "                y_hat = self.learner.predict(X)\n",
    "                loss = self.evaluator.get_loss(y,y_hat)\n",
    "                epoch_loss += loss.item()\n",
    "                for pred, answer in zip(torch.squeeze(y_hat).detach().numpy(), torch.squeeze(y).detach().numpy()):\n",
    "                    if answer == -1 and pred < 0.5:\n",
    "                        correct += 1\n",
    "                    elif answer == 1 and pred >= 0.5:\n",
    "                        correct += 1\n",
    "\n",
    "            self.log['test_loss'] += [epoch_loss / num_batches]\n",
    "            self.log['correct'] = correct\n",
    "            self.log['total'] = len(dataloader.dataset)\n",
    "\n",
    "    def run(self, n_epochs: int):\n",
    "        for t in range(n_epochs):\n",
    "            self.train()\n",
    "            self.test()\n",
    "            if (t+1) % 100 == 0:\n",
    "                print(f\"Epoch {t+1}\\n------------\")\n",
    "                print(f\"Train loss {self.log['train_loss'][-1]}\\nTest loss {self.log['test_loss'][-1]}\")\n",
    "        print(\"Done!\")\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
